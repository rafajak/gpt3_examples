{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r-search QA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafajak/gpt3_examples/blob/master/r_search_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D71vY3PWZMe0",
        "outputId": "08ff40b1-b51c-46c6-add9-f9ebe711c894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install openai\n",
        "!pip install transformers\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import json"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/2d/b3bc2aa663b2c376f073fd141e128ecfb47f3aff95ccee284a74d85a1ef8/openai-0.2.6.tar.gz (157kB)\n",
            "\r\u001b[K     |██                              | 10kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.2.6-cp36-none-any.whl size=170808 sha256=747db515b1f2ca0eeb9cacb9a45b8204ad07607d70dc640b4ea9131c9205166b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/fe/38/5d427b7efc6ff4508b39945808cd4db1bbe106421960b4f0e3\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.2.6\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 13.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9ee3ae30fa5b4f6d4eb08232f499b1deb5aabc64dbebf1a33f25217db5699e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QhqScVBZcK6",
        "outputId": "e2e1315b-0354-4846-eb90-b3ef63c9aada",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# To keep your secret API key secure - create a api_key.json file containing the key and upload it with the function below. \n",
        "# My config file looks like this: {\"api_key\": \"heresmysecretkey\"}\n",
        "\n",
        "upload = files.upload()\n",
        "openai.api_key = json.load(open(\"api_key.json\", \"r\"))[\"api_key\"]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bbdacb58-bf37-44a0-9d0f-171d1f8560aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bbdacb58-bf37-44a0-9d0f-171d1f8560aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving api_key.json to api_key.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFz1MkxJaK7u"
      },
      "source": [
        "# Step 1: The user asks a question\n",
        "# Step 2: Use Semantic Search to give a score for how well each section of a document relates to the question\n",
        "# Step 3: Order the text blocks by score\n",
        "# Step 4: Insert the highest-scoring text block into a Q&A prompt along with the question\n",
        "# Step 5: Send the prompt to the API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkVRdySP1Ue-"
      },
      "source": [
        "document_raw = '''Key concepts:\n",
        "There are three concepts that are core to the API: prompt, completion, and tokens. The “prompt” is text input to the API, and the “completion” is the text that the API generates based on the prompt. For example, if you give the API the prompt, “As Descartes said, I think, therefore”, it will return the completion “ I am” with high probability. It’s worth noting that the API is stochastic by default, meaning that every time you call it you might get a slightly different completion, even if the prompt stays the same.\n",
        "\n",
        "The best way to get started exploring the API is with the Playground. It's simply a text box where you write the prompt and click the \"Submit\" button to generate the completion. \n",
        "\n",
        "You’ll probably see a few more words than that being generated, since the default response length setting for the playground is much higher. “Tokens”, which can be thought of as pieces of words. (Much like a jigsaw puzzle, the pieces are not cut up nicely according to the actual images displayed). The API turns text into tokens before processing it, as a trick to be able to handle more text at once. For example, the word “Descartes” gets broken up into the tokens “Desc”, “art” and “es”, while a short and common word like “pear” is a single token. One thing you’ll notice is that many tokens start with a whitespace, for example “ hello” and “ bye”.\n",
        "\n",
        "One limitation to keep in mind is that combined, the text prompt and generated completion must be below 2048 tokens (roughly ~1500 words).\n",
        "\n",
        "The API runs models with weights from the GPT-3 family with many speed and throughput improvements. We currently offer the following models: davinci, curie, babbage and ada. The models provide a spectrum of capability, where davinci is the most capable model and ada is the fastest. We recommend you use davinci when you're experimenting, since it will give the best completions. Once you’re ready to move your use case to production, we encourage you to try the other models to see if you get the same results but with lower latency.\n",
        "###\n",
        "Prompt Design 101:\n",
        "The API is capable of doing a variety of different tasks. The prompt is the text you send to the API to get it to generate a response. The response, called a “completion”, is what the API thinks is the logical continuation of the prompt. A well-written prompt provides enough information for the API to know what you are asking for and how it should respond.\n",
        "\n",
        "The best way to learn how to use it and find inspiration is to look at different examples and see how they work.\n",
        "\n",
        "These examples in this document include links that open up Playground demonstrations where you can interact and experiment with changing their contents. You can also click on the categories in the table below that jump to the section that describe how to create prompts for those tasks.\n",
        "###\n",
        "Prompt basics:\n",
        "The API can do everything from generate original stories to perform complex text analysis. Because it can do so many things you have to be explicit in showing it what you want.\n",
        "\n",
        "Showing, not just “telling”, is the secret to a good prompt. Even people familiar with machine learning accustomed to chatbots and single-purpose text models can get confused by this. The API’s power is its adaptability. The key to unlocking this adaptability is in learning how to show it what you want.\n",
        "\n",
        "The API tries to guess what you want from the prompt. If you send it the words “Give me a list of cat breeds,” the API wouldn’t automatically assume that you’re asking for a list of cat breeds. You could just as easily be asking the API to continue a conversation where the first words are “Give me a list of cat breeds” and the next ones are “and I’ll tell you which ones I like.” If the API only assumed that you wanted a list of cats it wouldn’t be as good at content creation, classification or other tasks.\n",
        "\n",
        "There are three simple guidelines to creating prompts:\n",
        "\n",
        "1. Show and tell Make it clear to the API what you want either through instructions, examples or a combination of the two. If you want the API to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that’s what you want.\n",
        "\n",
        "2. Provide quality data If you’re trying to build a classifier or get the API to follow a pattern, make sure that there are enough examples. Proofread your examples and check that it’s clear that there’s enough data for the API to create a response. The API is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
        "\n",
        "3. Check your settings The temperature and top_p settings control how deterministic the API is in generating a response. If you’re asking the API to provide you with a response where there’s only one right answer, then you’d want to set these lower. If you’re looking for a response that’s not obvious, then you might want to set them higher. The number one mistake people use with these settings is assuming that they’re “cleverness” or “creativity” controls.\n",
        "###\n",
        "Prompt troubleshooting:\n",
        "If you’re having trouble getting the API to perform as expected, follow this checklist:\n",
        "\n",
        "Is it clear what the intended generation should be?\n",
        "Are there enough examples?\n",
        "Did you check your examples for mistakes? (The API won’t tell you directly)\n",
        "Are you using temp and top_p correctly?\n",
        "###\n",
        "Classification:\n",
        "To create a text classifier with the API we provide a description of the task and provide a few examples. \n",
        "\n",
        "It’s worth paying attention to several features in this example:\n",
        "\n",
        "1. State what the prompt does at the start At the start of the example we state in plain language what the classifier does: “This is a tweet sentiment classifier.” By stating this up front it helps the API understand much more quickly what the goal of the response is supposed to be and you’ll end needing to provide fewer examples.\n",
        "\n",
        "2. Use plain language to describe your inputs and outputs We use plain language for the input “Tweet” and the expected output “Sentiment.” For best practices, start with plain language descriptions. While you can often use shorthand or keys to indicate the input and output, when building your prompt it’s best to start by being as descriptive as possible and then working backwards removing extra words as long as the performance to the prompt is consistent.\n",
        "\n",
        "3. Use separators to define your examples We use triple “#” as a separator between examples. You can use other characters or line breaks, but  triple “#” works pretty consistently and is also an easy to use stop sequence. Whatever separator you use, make sure that it’s clear to the API where an example starts and stops.\n",
        "\n",
        "4. Show the API how to respond to any case In this example we provide multiple outcomes “Positive”, “Negative” and “Neutral.” A neutral outcome is important because there will be many cases where even a human would have a hard time determining if something is positive or negative and situations where it’s neither.\n",
        "\n",
        "5. You can use text and emoji The classifier is a mix of text and emoji 👍. The API reads emoji and can even convert expressions to and from them.\n",
        "\n",
        "6. You need fewer examples for familiar tasks For this classifier we only provided a handful of examples. This is because the API already has an understanding of sentiment and the concept of a tweet. If you’re building a classifier for something the API might not be familiar with, it might be necessary to provide more examples.\n",
        "###\n",
        "Improving the classifier’s efficiency:\n",
        "\n",
        "After showing the API how tweets are classified by sentiment we then provide it a list of tweets and then a list of sentiment ratings with the same number index. The API is able to pick up from the first example how a tweet is supposed to be classified. In the second example it sees how to apply this to a list of tweets. This allows the API to rate five (and even more) tweets in just one API call.\n",
        "\n",
        "It’s important to note that when you ask the API to create lists or evaluate text you need to pay extra attention to your probability settings (Top P or Temperature) to avoid drift.\n",
        "\n",
        "Make sure your probability setting is calibrated correctly by running multiple tests.\n",
        "\n",
        "Don’t make your list too long or the API is likely to drift.\n",
        "###\n",
        "Generation:\n",
        "One of the most powerful yet simplest tasks to accomplish with the API is generating new ideas and versions or input. You can give the API a list of story ideas and it will add to that list from just a few examples. It can create business plans, character descriptions and marketing slogans just by providing it a handful of examples.\n",
        "\n",
        "All those this is a very simple prompt, there are several details worth noting:\n",
        "\n",
        "1. We explained the intent of the list Just like with the classifier, we tell the API up front what the list is about. This helps it focus on completing the list and not trying to guess what the pattern is behind it.\n",
        "\n",
        "2. Our example sets the pattern for the rest of the list Because we provided a one-sentence description, the API is going to try to follow that pattern for the rest of the items it adds to the list. If we want a more verbose response we need to set that up from the start.\n",
        "\n",
        "3. We prompt the API by adding an incomplete entry When the API sees “2.” and the prompt abruptly ends, the first thing it tries to do is figure out what should come after it. Since we already had an example with number one and gave the list a title, the most obvious response is to continue adding items to the list.\n",
        "###\n",
        "Advanced generation techniques: \n",
        "You can improve the quality of the responses by making a longer more diverse list in your prompt. One way to do that is to start off with one example, let the API generate more and select the ones that you like best and add them to the list. A few more high-quality variations can dramatically improve the quality of the responses.\n",
        "###\n",
        "Conversation:\n",
        "The API is extremely adept at carrying on conversations with humans and even with itself. With just a few lines of instruction the API can perform as a customer service chatbot that intelligently answers questions without ever getting flustered or a wise-cracking conversation partner that makes jokes and puns. The key is to tell the API how it should behave and then provide a few examples.\n",
        "\n",
        "1. We tell the API the intent but we also tell it how to behave Just like the other prompts, we cue the API into what the example represents, but we also add another key detail: we give it explicit instructions on how to interact with the phrase “The assistant is helpful, creative, clever, and very friendly.”\n",
        "\n",
        "Without that instruction the API might stray and mimic the human it’s interacting with and become sarcastic or some other behavior we want to avoid.\n",
        "\n",
        "2. We give the API an identity At the start we have the API respond as an AI that was created by OpenAI. While the API has no intrinsic identity, this helps it respond in a way that’s as close to the truth as possible. You can use identity in other ways to create other kinds of chatbots. If you tell the API to respond as a woman who works as a research scientist in biology, you’ll get intelligent and thoughtful comments from the API similar to what you’d expect from someone with that background.\n",
        "###\n",
        "Transformation:\n",
        "The API is a language model that is familiar with a variety of ways that words and characters can be used to express information. This ranges from natural language text to computer code and languages other than English. The API is also able to understand content on a level that allows it to summarize, convert and express it in different ways.\n",
        "###\n",
        "Translation:\n",
        "In this example we show the API how to convert from English to French:\n",
        "<hr>\n",
        "English: I do not speak French.\n",
        "\n",
        "French: Je ne parle pas français.\n",
        "\n",
        "English: See you later!\n",
        "\n",
        "French: À tout à l'heure!\n",
        "\n",
        "English: Where is a good restaurant?\n",
        "\n",
        "French: Où est un bon restaurant?\n",
        "\n",
        "English: What rooms do you have available?\n",
        "\n",
        "French: Quelles chambres avez-vous de disponible?\n",
        "\n",
        "English:\n",
        "<hr>\n",
        "This example works because the API already has a grasp of French, so there’s no need to try to teach it this language. Instead, we just need to provide enough examples that API understands that it’s converting from one language to another.\n",
        "\n",
        "If you want to translate from English to a language the API is unfamiliar with you’d need to provide it with more examples and a fine-tuned model to do it fluently.\n",
        "###\n",
        "Conversion:\n",
        "In this example we convert the name of a movie into emoji. This shows the adaptability of the API to picking up patterns and working with other characters.\n",
        "<hr>\n",
        "Back to Future: 👨👴🚗🕒\n",
        "\n",
        "Batman: 🤵🦇\n",
        "\n",
        "Transformers: 🚗🤖\n",
        "\n",
        "Wonder Woman: 👸🏻👸🏼👸🏽👸🏾👸🏿\n",
        "\n",
        "Spider-Man: 🕸🕷🕸🕸🕷🕸\n",
        "\n",
        "Winnie the Pooh: 🐻🐼🐻\n",
        "\n",
        "The Godfather: 👨👩👧🕵🏻‍♂️👲💥\n",
        "\n",
        "Game of Thrones: 🏹🗡🗡🏹\n",
        "\n",
        "Spider-Man:\n",
        "<hr>\n",
        "###\n",
        "Summarization:\n",
        "The API is able to grasp the context of text and rephrase it in different ways. In this example the API takes a block of text and creates an explanation a child would understand. This illustrates that the API has a deep grasp of language.\n",
        "<hr>\n",
        "My ten-year-old asked me what this passage means:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.1 Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars.2 Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.3 They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "I rephrased it for him, in plain language a ten-year-old can understand:\n",
        "\n",
        "\"\"\"\n",
        "<hr>\n",
        "\n",
        "In this example we place whatever we want summarized between the triple quotes. It’s worth noting that we explain both before and after the text to be summarized what our intent is and who the target audience is for the summary. This is to keep the API from drifting after it processes a large block of text.\n",
        "###\n",
        "Factual responses:\n",
        "The API has a lot of knowledge that it’s learned from the data that it was been trained on. It also has the ability to provide responses that sound very real but are in fact made up. There are two ways to limit the likelihood of the API making up an answer.\n",
        "\n",
        "1. Provide a ground truth for the API If you provide the API with a body of text to answer questions about (like a Wikipedia entry) it will be less likely to confabulate a response.\n",
        "\n",
        "2. Use a low probability and show the API how to say “I don’t know” If the API understands that in cases where it’s less certain about a response that saying “I don’t know” or some variation is appropriate, it will be less inclined to make up answers.\n",
        "###\n",
        "Completion:\n",
        "While all prompts are forms of completions it can be helpful to think of text completion as its own task in instances where you want the API to pick up where you left off. Examples of this include helping you write a summary or writing lines of code where the API can infer what should come next. \n",
        "\n",
        "The API already has an understanding of the React library and is able to continue the rest of the code once it has an example of what the user is trying to create.\n",
        "\n",
        "In this next prompt the API is able to determine the intent of the writer and help continue the train of thought about vertical farming. It’s also an example of where the probability setting will keep the API focused on the intent of the prompt or let it go off on a tangent.\n",
        "\n",
        "<hr>\n",
        "Vertical farming provides a novel solution for producing food locally, reducing transportation costs and\n",
        "<hr>\n",
        "###\n",
        "Factual responses:\n",
        "The API has a lot of knowledge that it’s learned from the data that it was been trained on. It also has the ability to provide responses that sound very real but are in fact made up. There are two ways to limit the likelihood of the API making up an answer.\n",
        "\n",
        "1. Provide a ground truth for the API If you provide the API with a body of text to answer questions about (like a Wikipedia entry) it will be less likely to confabulate a response.\n",
        "\n",
        "2. Use a low probability and show the API how to say “I don’t know” If the API understands that in cases where it’s less certain about a response that saying “I don’t know” or some variation is appropriate, it will be less inclined to make up answers.\n",
        "###\n",
        "Semantic search:\n",
        "The API lets you do semantic search over documents. This means that you can provide a query, such as a natural language question or a statement, and find documents that answer the question or are semantically related to the statement. The “documents” can be words, sentences, paragraphs or even longer documents. For example, if you provide documents [\"White House\", \"hospital\", \"school\"] and query \"the president\", you’ll get a different similarity score for each document. The higher the similarity score, the more semantically similar the document is to the query (in this example, “White House” will be most similar to “the president”).\n",
        "\n",
        "Each search query produces a different distribution of scores for a fixed group of documents. For instance, if you have a group of documents that are summaries of books, the query \"sci-fi novels\" might have a mean score of 150 and standard deviation of 50, whereas the query \"cat training\" might have a mean score of 200 and standard deviation of 10, if you were to search these queries against every document in the group. The variation is a consequence of the search setup, where the query's probability (what is used to create the score) is conditioned on the document's probability.\n",
        "\n",
        "If you need scores that don't vary by query, you can randomly sample 50-100 documents for a query and calculate the mean and standard deviation, then normalize new scores for that same query using that mean and standard deviation.\n",
        "\n",
        "The similarity score is a positive score that usually ranges from 0 to 300 (but can sometimes go higher), where a score above 200 usually means the document is semantically similar to the query. At the moment, the score is very useful for ranking (we’ve seen it outperform many existing semantic ranking approaches). For example, you can use it for re-ranking the top few hundred examples from an existing information retrieval system.\n",
        "\n",
        "One thing to keep in mind for semantic search is the tradeoff between model accuracy and speed, as speed is often a desired property of search. We’ve found the “ada” model sufficient for many search tasks, and it’s substantially faster than the most capable model, “davinci”. We encourage you to experiment with the different models to see if “ada” or “babbage” work for your search use case.\n",
        "\n",
        "The search endpoint can query up to 200 documents in one request. If you have more documents than that, you can divide them over multiple requests (the document similarly scores will stay the same across requests as the query stays the same). One limitation to keep in mind is that the query and longest document must be below 2000 tokens together. You can read more about the search endpoint in the API Reference.\n",
        "###'''"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_qET0kt1_Re"
      },
      "source": [
        "documents = [doc.strip() for doc in document_raw.split(\"###\") if doc]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RWhz9oU2O52"
      },
      "source": [
        "question = \"How do I simplify text?\""
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkDzKwpQ1kZ1"
      },
      "source": [
        "response = openai.Engine(\"davinci\").search(\n",
        "  documents=documents,\n",
        "  query=question\n",
        ")"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtxRYTtI2_CX"
      },
      "source": [
        "response_df = pd.concat([pd.DataFrame(response[\"data\"]),\n",
        "                         pd.Series(documents, name= \"documents\")],axis=1)\n",
        "response_df = response_df[[\"documents\", \"score\"]]\n",
        "\n",
        "response_df = response_df.sort_values(by=\"score\",ascending=False)\n",
        "\n",
        "top_document = response_df.iloc[0][\"documents\"]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF1lYhDA_Pzo"
      },
      "source": [
        "context_a = \"\"\"I was asked to read through this documentation and answer the following question:\n",
        "-----\n",
        "Classification:\n",
        "To create a text classifier with the API we provide a description of the task and provide a few examples. \n",
        "\n",
        "It’s worth paying attention to several features in this example:\n",
        "\n",
        "1. State what the prompt does at the start At the start of the example we state in plain language what the classifier does: “This is a tweet sentiment classifier.” By stating this up front it helps the API understand much more quickly what the goal of the response is supposed to be and you’ll end needing to provide fewer examples.\n",
        "\n",
        "2. Use plain language to describe your inputs and outputs We use plain language for the input “Tweet” and the expected output “Sentiment.” For best practices, start with plain language descriptions. While you can often use shorthand or keys to indicate the input and output, when building your prompt it’s best to start by being as descriptive as possible and then working backwards removing extra words as long as the performance to the prompt is consistent.\n",
        "\n",
        "3. Use separators to define your examples We use triple “#” as a separator between examples. You can use other characters or line breaks, but  triple “#” works pretty consistently and is also an easy to use stop sequence. Whatever separator you use, make sure that it’s clear to the API where an example starts and stops.\n",
        "\n",
        "4. Show the API how to respond to any case In this example we provide multiple outcomes “Positive”, “Negative” and “Neutral.” A neutral outcome is important because there will be many cases where even a human would have a hard time determining if something is positive or negative and situations where it’s neither.\n",
        "\n",
        "5. You can use text and emoji The classifier is a mix of text and emoji 👍. The API reads emoji and can even convert expressions to and from them.\n",
        "\n",
        "6. You need fewer examples for familiar tasks For this classifier we only provided a handful of examples. This is because the API already has an understanding of sentiment and the concept of a tweet. If you’re building a classifier for something the API might not be familiar with, it might be necessary to provide more examples.\n",
        "\n",
        "-----\n",
        "This is the question I was asked:\n",
        "-----\n",
        "Can I use this to categorize items in text?\n",
        "-----\n",
        "This is my simple answer to the question based on the documentation:\n",
        "-----\n",
        "Yes. Classification allows you to apply categories Tweets, emojis and any other kind of text.\n",
        "#####\"\"\"\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR59GY6rA5Uy"
      },
      "source": [
        "context_b = f\"\"\"I was asked to read through this documentation and answer the following question:\n",
        "-----\n",
        "{top_document}\n",
        "-----\n",
        "This is the question I was asked:\n",
        "-----\n",
        "{question}\n",
        "-----\n",
        "This is my simple answer to the question based on the documentation:\n",
        "-----\\n\"\"\""
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4801ndBMtN"
      },
      "source": [
        "prompt = context_a + context_b"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFnmqeSjGYes",
        "outputId": "1b1c4c48-2209-42b6-eb96-89959f049a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "prompt"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I was asked to read through this documentation and answer the following question:\\n-----\\nClassification:\\nTo create a text classifier with the API we provide a description of the task and provide a few examples. \\n\\nIt’s worth paying attention to several features in this example:\\n\\n1. State what the prompt does at the start At the start of the example we state in plain language what the classifier does: “This is a tweet sentiment classifier.” By stating this up front it helps the API understand much more quickly what the goal of the response is supposed to be and you’ll end needing to provide fewer examples.\\n\\n2. Use plain language to describe your inputs and outputs We use plain language for the input “Tweet” and the expected output “Sentiment.” For best practices, start with plain language descriptions. While you can often use shorthand or keys to indicate the input and output, when building your prompt it’s best to start by being as descriptive as possible and then working backwards removing extra words as long as the performance to the prompt is consistent.\\n\\n3. Use separators to define your examples We use triple “#” as a separator between examples. You can use other characters or line breaks, but  triple “#” works pretty consistently and is also an easy to use stop sequence. Whatever separator you use, make sure that it’s clear to the API where an example starts and stops.\\n\\n4. Show the API how to respond to any case In this example we provide multiple outcomes “Positive”, “Negative” and “Neutral.” A neutral outcome is important because there will be many cases where even a human would have a hard time determining if something is positive or negative and situations where it’s neither.\\n\\n5. You can use text and emoji The classifier is a mix of text and emoji 👍. The API reads emoji and can even convert expressions to and from them.\\n\\n6. You need fewer examples for familiar tasks For this classifier we only provided a handful of examples. This is because the API already has an understanding of sentiment and the concept of a tweet. If you’re building a classifier for something the API might not be familiar with, it might be necessary to provide more examples.\\n\\n-----\\nThis is the question I was asked:\\n-----\\nCan I use this to categorize items in text?\\n-----\\nThis is my simple answer to the question based on the documentation:\\n-----\\nYes. Classification allows you to apply categories Tweets, emojis and any other kind of text.\\n#####I was asked to read through this documentation and answer the following question:\\n-----\\nSummarization:\\nThe API is able to grasp the context of text and rephrase it in different ways. In this example the API takes a block of text and creates an explanation a child would understand. This illustrates that the API has a deep grasp of language.\\n<hr>\\nMy ten-year-old asked me what this passage means:\\n\\n\"\"\"\\n\\nA neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.1 Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars.2 Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.3 They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.\\n\\n\"\"\"\\n\\nI rephrased it for him, in plain language a ten-year-old can understand:\\n\\n\"\"\"\\n<hr>\\n\\nIn this example we place whatever we want summarized between the triple quotes. It’s worth noting that we explain both before and after the text to be summarized what our intent is and who the target audience is for the summary. This is to keep the API from drifting after it processes a large block of text.\\n-----\\nThis is the question I was asked:\\n-----\\nHow do I simplify text?\\n-----\\nThis is my simple answer to the question based on the documentation:\\n-----\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9GnqmEQ98nJ",
        "outputId": "8d1558f9-7e6c-44da-d641-7bc9976b2e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kwargs = {\"engine\":\"davinci\",\n",
        "          \"temperature\": 0.7,\n",
        "          \"max_tokens\": 10,\n",
        "          \"echo\": False}\n",
        "\n",
        "r = openai.Completion.create(prompt=prompt, **kwargs)\n",
        "\n",
        "print(r[\"choices\"][0][\"text\"])"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The API can summarize text and pull out key words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAt9TEwWF5tt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNIXEwQB3oFb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLcftQpvTi6"
      },
      "source": [
        "# def rgpt(query, docs):\n",
        "#     search_docs = openai.search(query, docs)\n",
        "#     context = \"\"\n",
        "#     for doc in sorted(search_docs):\n",
        "#           if len(context) + len(doc) < max_context_length:\n",
        "# \t       context += doc\n",
        "#     return openai.Completion.create(prompt=context+question)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}